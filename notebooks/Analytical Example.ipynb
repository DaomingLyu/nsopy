{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nsopy: basic usage examples\n",
    "\n",
    "Generally, the inputs required are\n",
    "\n",
    "* a first-order oracle of the problem: for a given $x_k \\in \\mathbb{X} \\subseteq \\mathbb{R}^n$, it returns $f(x_k)$ and a valid subgradient $\\nabla f(x_k)$,\n",
    "* the projection function $\\Pi_{\\mathbb{X}}: \\mathbb{R}^n \\rightarrow \\mathbb{R}^n$.\n",
    "\n",
    "## Example 1: simple analytical example\n",
    "\n",
    "Consider the following problem from [1, Sec. 2.1.3]:\n",
    "$$\n",
    "\\begin{array}{ll}\n",
    " \\min & f(x_1, x_2) = \\left\\{ \\begin{array}{ll} \n",
    "             5(9x_1^2 + 16x_2^2)^{1/2} & \\mathrm{if \\ } x_1 > \\left|x_2\\right|, \\\\\n",
    "             9x_1 + 16\\left|x_2\\right| & \\mathrm{if \\ } x_1 \\leq \\left|x_2\\right| \\\\\n",
    " \\end{array} \\right.\\\\\n",
    " \\mathrm{s.t.} & -3 \\leq x_1, x_2 \\leq 3.\n",
    "\\end{array}\n",
    "$$\n",
    "![alt text](img/ex_1.png)\n",
    "\n",
    "This problem is interesting because a common gradient algorithm with backtracking initialized anywhere where in the set\n",
    "$\\left\\{(x_1,x_2) \\left| \\ x_1 > \\left|x_2\\right| > (9/16)^2\\left|x_1\\right| \\right. \\right\\}$\n",
    "fails to converge to the optimum (-3,0), by remaining stuck at (0,0), even though it never touches any point where the function is nondifferentiable, see discussion in [1, Sec. 2.1.3]. Here we test our methods on this problem.\n",
    "\n",
    "We write the oracle and projection as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vujanicr\\PycharmProjects\\nsopy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib notebook\n",
    "\n",
    "%cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Note**: all methods have been devised to solve the maximization of concave functions. To minimize (as in this case), we just need to negate the oracle's returns, i.e., the objective value $f(x_k)$ and the subgradient $\\nabla f(x_k)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def oracle(x):\n",
    "    assert -3 <= x[0] <= 3, 'oracle must be queried within X'\n",
    "    assert -3 <= x[1] <= 3, 'oracle must be queried within X'\n",
    "    # compute function value a subgradient\n",
    "    if x[0] > abs(x[1]):\n",
    "        f_x = 5*(9*x[0]**2 + 16*x[1]**2)**(float(1)/float(2))\n",
    "        diff_f_x = np.array([float(9*5*x[0])/np.sqrt(9*x[0]**2 + 16*x[1]**2), \n",
    "                             float(16*5*x[0])/np.sqrt(9*x[0]**2 + 16*x[1]**2)])\n",
    "    else:\n",
    "        f_x = 9*x[0] + 16*abs(x[1])\n",
    "        if x[1] >= 0:\n",
    "            diff_f_x = np.array([9, 16], dtype=float)\n",
    "        else:\n",
    "            diff_f_x = np.array([9, -16], dtype=float)\n",
    "    return 0, -f_x, -diff_f_x  # return negation to minimize\n",
    "\n",
    "\n",
    "def projection_function(x):\n",
    "    # projection on the box is simply saturating the entries\n",
    "    return np.array([min(max(x[0],-3),3), min(max(x[1],-3),3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now solve it by applying one of the several methods available:\n",
    "\n",
    "**Note**: try to change the method and see for yourself how their trajectories differ!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nsopy import SGMDoubleSimpleAveraging as DSA\n",
    "from nsopy import SGMTripleAveraging as TA\n",
    "from nsopy import SubgradientMethod as SG\n",
    "from nsopy import UniversalPGM as UPGM\n",
    "from nsopy import UniversalDGM as UDGM\n",
    "from nsopy import UniversalFGM as UFGM\n",
    "from nsopy import GenericDualMethodLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# method = DSA(oracle, projection_function, dimension=2, gamma=0.5)\n",
    "# method = TA(oracle, projection_function, dimension=2, variant=2, gamma=0.5)\n",
    "# method = SG(oracle, projection_function, dimension=2)\n",
    "method = UPGM(oracle, projection_function, dimension=2, epsilon=10)\n",
    "# method = UDGM(oracle, projection_function, dimension=2, epsilon=1.0)\n",
    "# method = UFGM(oracle, projection_function, dimension=2, epsilon=1.0)\n",
    "\n",
    "method_logger = GenericDualMethodLogger(method)\n",
    "# start from an different initial point\n",
    "x_0 = np.array([2.01,2.01])\n",
    "method.lambda_k = x_0\n",
    "\n",
    "for iteration in range(500):\n",
    "    method.dual_step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally plot the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "box = np.linspace(-3, 3, 31)\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.plot_trisurf(np.array([x_1 for x_1 in box for x_2 in box]),\n",
    "                np.array([x_2 for x_1 in box for x_2 in box]),\n",
    "                np.array([-oracle([x_1, x_2])[1] for x_1 in box for x_2 in box]))\n",
    "ax.set_xlabel('$x_1$')\n",
    "ax.set_ylabel('$x_2$')\n",
    "ax.set_zlabel('$f(x)$')\n",
    "\n",
    "plt.plot([x[0] for x in method_logger.lambda_k_iterates],\n",
    "         [x[1] for x in method_logger.lambda_k_iterates],\n",
    "         [-f_x for f_x in method_logger.d_k_iterates], 'r.-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "[1] Dimitri Bertsekas, Convex Optimization Algorithms, Athena Scientific Belmont, 2015."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
